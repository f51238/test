import pandas as pd
import numpy as np
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from textblob import TextBlob

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')

# Preprocessing function
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove stop words and punctuation
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text)
    filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]

    # Stem the words
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]

    # Rejoin the tokens into a string
    preprocessed_text = ' '.join(stemmed_tokens)
    return preprocessed_text

# Apply preprocessing to the comments
data['comment'] = data['comment'].apply(preprocess_text)

comments = data['comment']  # Replace 'comment' with the actual column name containing the comments

# Use a pre-trained model from Hugging Face to categorize the comments
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

def categorize_comment(comment):
    inputs = tokenizer.encode_plus(
        comment,
        max_length=512,
        truncation=True,
        padding='max_length',
        return_tensors='pt'
    )
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    category_idx = logits.argmax(dim=-1).item()
    category = model.config.id2label[category_idx]
    return category

data['category'] = data['comment'].apply(categorize_comment)

# Function to extract noun phrases using TextBlob
def extract_noun_phrases(text):
    blob = TextBlob(text)
    noun_phrases = blob.noun_phrases
    return noun_phrases

# Apply the function to extract noun phrases from the comments
data['noun_phrases'] = data['comment'].apply(extract_noun_phrases)

# Perform clustering to group similar comments and extract root problems
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(comments)

n_clusters = 3  # Adjust the number of clusters based on your domain knowledge
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
data['cluster'] = kmeans.fit_predict(X)

# Extract keywords and representative comments from each cluster to summarize the root problems
def extract_keywords(cluster):
    top_keywords = []
    for idx in range(len(cluster)):
        tfidf_scores = zip(vectorizer.get_feature_names(), X[idx].toarray()[0])
        sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:5]
        top_keywords.extend([keyword for keyword, score in sorted_tfidf_scores])

        # Add noun phrases to the keywords
        noun_phrases = cluster['noun_phrases'].iloc[idx]
        top_keywords.extend(noun_phrases)
    return set(top_keywords)

summary = []
for i in range(n_clusters):
    cluster_data = data[data['cluster'] == i]
    keywords = extract_keywords(cluster_data)
    representative_comment = cluster_data['comment'].iloc[0]
    summary.append((i, keywords, representative_comment))

for cluster, keywords, representative_comment in summary:
    print(f"Cluster {cluster}:")
    print(f"Keywords: {keywords}")
    print(f"Representative comment: {representative_comment}\n")
