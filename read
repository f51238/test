import pandas as pd
import numpy as np
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from textblob import TextBlob

# Load the data into a DataFrame (assuming it's in a CSV format)
# Sample comments data
data = pd.DataFrame({
    'comment': [
        "I cannot log in after returning from my vacation.",
        "My absence balance is incorrect after coming back from leave.",
        "I'm having trouble resetting my password.",
        "The platform won't let me update my profile picture.",
        "I'm unable to submit my expense report."
    ]
})

comments = data['comment']  # Replace 'comment' with the actual column name containing the comments

# Use a pre-trained model from Hugging Face to categorize the comments
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

nlp = pipeline("text-classification", model=model, tokenizer=tokenizer)

def categorize_comment(comment):
    result = nlp(comment)
    category = result[0]['label']
    return category

data['category'] = data['comment'].apply(categorize_comment)

# Function to extract noun phrases using TextBlob
def extract_noun_phrases(text):
    blob = TextBlob(text)
    noun_phrases = blob.noun_phrases
    return noun_phrases

# Apply the function to extract noun phrases from the comments
data['noun_phrases'] = data['comment'].apply(extract_noun_phrases)

# Perform clustering to group similar comments and extract root problems
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(comments)

n_clusters = 3  # Adjust the number of clusters based on your domain knowledge
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
data['cluster'] = kmeans.fit_predict(X)

# Extract keywords and representative comments from each cluster to summarize the root problems
def extract_keywords(cluster):
    top_keywords = []
    for idx in range(len(cluster)):
        tfidf_scores = zip(vectorizer.get_feature_names(), X[idx].toarray()[0])
        sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:5]
        top_keywords.extend([keyword for keyword, score in sorted_tfidf_scores])

        # Add noun phrases to the keywords
        noun_phrases = cluster['noun_phrases'].iloc[idx]
        top_keywords.extend(noun_phrases)
    return set(top_keywords)

summary = []
for i in range(n_clusters):
    cluster_data = data[data['cluster'] == i]
    keywords = extract_keywords(cluster_data)
    representative_comment = cluster_data['comment'].iloc[0]
    summary.append((i, keywords, representative_comment))

for cluster, keywords, representative_comment in summary:
    print(f"Cluster {cluster}:")
    print(f"Keywords: {keywords}")
    print(f"Representative comment: {representative_comment}\n")
